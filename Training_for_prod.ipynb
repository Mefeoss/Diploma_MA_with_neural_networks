{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9AsEN4VJ5U8"
      },
      "source": [
        "# Дипломная работа на тему: От нейросетевого анализа к словарному: методы разработки морфологических анализаторов на основе данных, размеченных нейросетью\n",
        "### Автор: _Феоктистова Эмма Александровна, 4 курс ФиКЛ_\n",
        "### Научный руководитель: _Проф. Ляшевская О. Н._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUsQmjGdJ5U-"
      },
      "source": [
        "## 0. Необходимые импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:03.472379Z",
          "start_time": "2023-05-26T20:29:03.457380Z"
        },
        "id": "s8lIRGoLJ5U-"
      },
      "outputs": [],
      "source": [
        "import conllu\n",
        "from conllu import parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:03.504380Z",
          "start_time": "2023-05-26T20:29:03.489380Z"
        },
        "id": "XglYGT18J5U_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:08.435433Z",
          "start_time": "2023-05-26T20:29:07.117789Z"
        },
        "id": "A0tSL9tCJ5U_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:12.328489Z",
          "start_time": "2023-05-26T20:29:08.467434Z"
        },
        "id": "ce1ZQtFAJ5U_"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:16.402241Z",
          "start_time": "2023-05-26T20:29:12.360490Z"
        },
        "id": "Cvw1GR8jJ5U_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.file_utils import cached_property\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:16.448241Z",
          "start_time": "2023-05-26T20:29:16.433241Z"
        },
        "id": "wxMSiyXfJ5VA"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPE4wctUJ5VI"
      },
      "source": [
        "## 2. T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:55.151629Z",
          "start_time": "2023-05-18T20:36:40.068984Z"
        },
        "id": "_QrsrQbRJ5VJ",
        "outputId": "cba1c418-b6ae-4abc-b304-3ebeb9def08a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers[torch]==4.3\n",
            "  Using cached transformers-4.3.0-py3-none-any.whl (1.8 MB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp39-cp39-win_amd64.whl (2.0 MB)\n",
            "Requirement already satisfied: sacremoses in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (0.0.53)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (1.22.3)\n",
            "Requirement already satisfied: requests in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (2022.4.24)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (4.64.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (3.10.2)\n",
            "Requirement already satisfied: torch>=1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers[torch]==4.3) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in c:\\programdata\\miniconda3\\lib\\site-packages (from torch>=1.0->transformers[torch]==4.3) (4.2.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]==4.3) (0.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging->transformers[torch]==4.3) (3.0.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers[torch]==4.3) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers[torch]==4.3) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers[torch]==4.3) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers[torch]==4.3) (3.3)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (1.2.0)\n",
            "Requirement already satisfied: six in c:\\programdata\\miniconda3\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (1.16.0)\n",
            "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (7.1.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "ERROR: Could not install packages due to an OSError: [WinError 5] Отказано в доступе: 'C:\\\\ProgramData\\\\Miniconda3\\\\Lib\\\\site-packages\\\\~%kenizers\\\\tokenizers.cp39-win_amd64.pyd'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ufal.udpipe in c:\\programdata\\miniconda3\\lib\\site-packages (1.3.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ruprompts in c:\\programdata\\miniconda3\\lib\\site-packages (0.1.4)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.10.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from ruprompts) (1.11.0+cu113)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.13.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from ruprompts) (2.13.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from ruprompts) (4.28.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from ruprompts) (4.2.0)\n",
            "Requirement already satisfied: torchtyping<0.2.0,>=0.1.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from ruprompts) (0.1.4)\n",
            "Requirement already satisfied: requests in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (2.27.1)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (3.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (2022.4.24)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (1.22.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (21.3)\n",
            "Requirement already satisfied: fsspec in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers<5.0.0,>=4.6.0->ruprompts) (2022.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->ruprompts) (3.0.8)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers<5.0.0,>=4.6.0->ruprompts) (0.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (1.26.9)\n",
            "Installing collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\programdata\\miniconda3\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: multiprocess in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: xxhash in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (1.22.3)\n",
            "Requirement already satisfied: responses<0.19 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -dm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\programdata\\miniconda3\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]==4.3\n",
        "!pip install ufal.udpipe\n",
        "!pip install ruprompts\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Указываем путь к используемым датасетам:"
      ],
      "metadata": {
        "id": "Pop43nH4KXjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:56.021267Z",
          "start_time": "2023-05-18T20:36:55.199926Z"
        },
        "id": "ZWSXggG6J5VJ",
        "outputId": "59381514-fa9e-48ff-ecba-11d3fa75ee99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>form</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Анкета</td>\n",
              "      <td>анкета,NOUN,Inan,Nom,Fem,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "      <td>.,PUNCT,None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Начальник</td>\n",
              "      <td>начальник,NOUN,Anim,Nom,Masc,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>областного</td>\n",
              "      <td>областной,ADJ,Gen,Pos,Neut,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>управления</td>\n",
              "      <td>управление,NOUN,Inan,Gen,Neut,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359887</th>\n",
              "      <td>внимание</td>\n",
              "      <td>внимание,NOUN,Inan,Nom,Neut,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359888</th>\n",
              "      <td>-</td>\n",
              "      <td>-,PUNCT,None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359889</th>\n",
              "      <td>большая</td>\n",
              "      <td>большой,ADJ,Nom,Pos,Fem,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359890</th>\n",
              "      <td>редкость</td>\n",
              "      <td>редкость,NOUN,Inan,Nom,Fem,Sing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359891</th>\n",
              "      <td>.</td>\n",
              "      <td>.,PUNCT,None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1359892 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               form                                data\n",
              "0            Анкета       анкета,NOUN,Inan,Nom,Fem,Sing\n",
              "1                 .                        .,PUNCT,None\n",
              "2         Начальник   начальник,NOUN,Anim,Nom,Masc,Sing\n",
              "3        областного     областной,ADJ,Gen,Pos,Neut,Sing\n",
              "4        управления  управление,NOUN,Inan,Gen,Neut,Sing\n",
              "...             ...                                 ...\n",
              "1359887    внимание    внимание,NOUN,Inan,Nom,Neut,Sing\n",
              "1359888           -                        -,PUNCT,None\n",
              "1359889     большая        большой,ADJ,Nom,Pos,Fem,Sing\n",
              "1359890    редкость     редкость,NOUN,Inan,Nom,Fem,Sing\n",
              "1359891           .                        .,PUNCT,None\n",
              "\n",
              "[1359892 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('./prepared_data/SynTagRus/syntagrus_train_data.csv')\n",
        "val_data = pd.read_csv('./prepared_data/SynTagRus/syntagrus_dev_data.csv')\n",
        "full_train_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "full_train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:56.085275Z",
          "start_time": "2023-05-18T20:36:56.070269Z"
        },
        "id": "2jK8MF3RJ5VJ"
      },
      "outputs": [],
      "source": [
        "class PairsDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        \"\"\"\n",
        "        x - dict; example (from toxic comment):\n",
        "            {\n",
        "                'input_ids': [[55, 27, 103, 172], [157, 24529, 4088, 2], ...],\n",
        "                'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], ...]\n",
        "            }\n",
        "        y - the same dict as x (from neutral comment)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        idx - index of current object\n",
        "        \n",
        "        returns dict:\n",
        "            {\n",
        "                'input_ids': [157, 24529, 4088, 2] # 'input_ids' from `x` for `idx\n",
        "                'attention_mask': [1, 1, 1, 1] # 'attention_mask' from `x` for `idx`\n",
        "                'decoder_attention_mask': [1, 1, 1, 1] # 'attention_mask' from `y` for `idx`\n",
        "                'labels': [422, 584, 17940, 246] # 'input_ids' from `y` for `idx`\n",
        "            }\n",
        "        \"\"\"\n",
        "        assert idx < len(self.x['input_ids']) # idx must be less than len of 'toxic' list (list from column `toxic_comment`)\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "    \n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n # * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:56.147275Z",
          "start_time": "2023-05-18T20:36:56.132268Z"
        },
        "id": "IMeSIjUOJ5VJ"
      },
      "outputs": [],
      "source": [
        "def cleanup():\n",
        "    \"\"\"\n",
        "    A helpful function to clean all cached batches.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбираем модель (мы использовали более маленькую модель _T5-small_ для быстроты обучения): "
      ],
      "metadata": {
        "id": "DykARe8_KsUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:56.208623Z",
          "start_time": "2023-05-18T20:36:56.196739Z"
        },
        "id": "iImQMdW-J5VJ"
      },
      "outputs": [],
      "source": [
        "# model_name = 'sberbank-ai/ruT5-base'\n",
        "model_name = 't5-small'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:58.241457Z",
          "start_time": "2023-05-18T20:36:56.256886Z"
        },
        "id": "TOcbqw65J5VJ",
        "outputId": "fabba760-a7bd-4508-dac7-5d5fc25a6c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем уже предобученный токенайзер и преобразуем данные:"
      ],
      "metadata": {
        "id": "qvhEJxTHKwvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:58.571528Z",
          "start_time": "2023-05-18T20:36:58.305490Z"
        },
        "id": "Wh-ibyvzJ5VK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:36:58.651532Z",
          "start_time": "2023-05-18T20:36:58.620527Z"
        },
        "id": "e4gi1LecJ5VK"
      },
      "outputs": [],
      "source": [
        "x, y = full_train_data['form'].tolist(), full_train_data['data'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:29.657562Z",
          "start_time": "2023-05-18T20:36:59.693527Z"
        },
        "id": "BDqMf2BxJ5VK"
      },
      "outputs": [],
      "source": [
        "dataset = PairsDataset(tokenizer(x), tokenizer(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:30.744525Z",
          "start_time": "2023-05-18T20:37:30.730527Z"
        },
        "scrolled": true,
        "id": "2j-q90FBJ5VK",
        "outputId": "29ca856e-92a0-41b1-c7b3-38b183a7c005"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [3, 2, 7184, 6652, 1757, 15517, 1],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
              " 'decoder_attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'labels': [3,\n",
              "  25873,\n",
              "  6652,\n",
              "  1757,\n",
              "  15517,\n",
              "  6,\n",
              "  7400,\n",
              "  7443,\n",
              "  6,\n",
              "  1570,\n",
              "  152,\n",
              "  6,\n",
              "  4168,\n",
              "  51,\n",
              "  6,\n",
              "  371,\n",
              "  15,\n",
              "  51,\n",
              "  6,\n",
              "  134,\n",
              "  53,\n",
              "  1]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__getitem__(idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем параметры обучающего датасета:"
      ],
      "metadata": {
        "id": "Q7H4PxuEK1a-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:31.819535Z",
          "start_time": "2023-05-18T20:37:31.805526Z"
        },
        "id": "IMi7npMlJ5VK"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset, \n",
        "    batch_size=8, \n",
        "    drop_last=False, \n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:35.383535Z",
          "start_time": "2023-05-18T20:37:32.907527Z"
        },
        "id": "uPzFnJKFJ5VK"
      },
      "outputs": [],
      "source": [
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции для оценки модели и ее обучения с настраиваемыми параметрами:"
      ],
      "metadata": {
        "id": "oDyjhyhUK5LE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:36.425532Z",
          "start_time": "2023-05-18T20:37:36.411527Z"
        },
        "id": "DJ2X4tcPJ5VK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dataloader):\n",
        "    num = 0\n",
        "    den = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        with torch.no_grad():\n",
        "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
        "            num += len(batch) * loss.item()\n",
        "            den += len(batch)\n",
        "    val_loss = num / den\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:37.439532Z",
          "start_time": "2023-05-18T20:37:37.424526Z"
        },
        "id": "NxlSA_pWJ5VL"
      },
      "outputs": [],
      "source": [
        "def train_loop(\n",
        "    model, train_dataloader, val_dataloader, \n",
        "    max_epochs=30, \n",
        "    max_steps=1_000, \n",
        "    lr=3e-5,\n",
        "    gradient_accumulation_steps=1, \n",
        "    cleanup_step=100,\n",
        "    report_step=300,\n",
        "    window=100,\n",
        "):\n",
        "    cleanup()\n",
        "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
        "\n",
        "    ewm_loss = 0\n",
        "    step = 0\n",
        "    model.train()\n",
        "\n",
        "    for epoch in trange(max_epochs):\n",
        "        print(step, max_steps)\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        tq = tqdm(train_dataloader)\n",
        "        for i, batch in enumerate(tq):\n",
        "            try:\n",
        "                batch['labels'][batch['labels']==0] = -100\n",
        "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
        "                loss.backward()\n",
        "            except Exception as e:\n",
        "                print('error on step', i, e)\n",
        "                loss = None\n",
        "                cleanup()\n",
        "                continue\n",
        "            if i and i % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                step += 1\n",
        "                if step >= max_steps:\n",
        "                    break\n",
        "\n",
        "            if i % cleanup_step == 0:\n",
        "                cleanup()\n",
        "\n",
        "            w = 1 / min(i+1, window)\n",
        "            ewm_loss = ewm_loss * (1-w) + loss.item() * w # for averaging loss values\n",
        "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
        "\n",
        "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
        "                model.eval()\n",
        "                eval_loss = evaluate_model(model, val_dataloader)\n",
        "                model.train()\n",
        "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
        "                \n",
        "            if step % 1000 == 0:\n",
        "                model.save_pretrained(f't5_base_{dname}_{steps}')\n",
        "        \n",
        "    cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:38.473534Z",
          "start_time": "2023-05-18T20:37:38.458527Z"
        },
        "id": "xXKQQTmfJ5VL"
      },
      "outputs": [],
      "source": [
        "def train_model(x, y, model_name, test_size=0.1, batch_size=8, **kwargs):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
        "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
        "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
        "    \n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
        "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:39.551534Z",
          "start_time": "2023-05-18T20:37:39.536527Z"
        },
        "id": "3nQHJBG6J5VL"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'train': full_train_data\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем _DataCollatorWithPadding_ для приведения наших данных в единный формат (в задачах NLP входные данные  обычно представляют собой последовательности токенов разной длины, данный класс приводит их к одной длине с помощью паддингов)."
      ],
      "metadata": {
        "id": "eeZS2r9tK-Em"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T20:37:40.595536Z",
          "start_time": "2023-05-18T20:37:40.580527Z"
        },
        "id": "wlZMaHgdJ5VL"
      },
      "outputs": [],
      "source": [
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        features example:\n",
        "            [    \n",
        "                {\"foo\": [1, 2, 3], \"bar\": torch.tensor([0.1, 0.2, 0.3])},\n",
        "                {\"foo\": [4, 5, 6], \"bar\": torch.tensor([0.4, 0.5, 0.6])},\n",
        "                {\"foo\": [7, 8, 9], \"bar\": torch.tensor([0.7, 0.8, 0.9])},\n",
        "            ]\n",
        "        \"\"\"\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        ) \n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "        \n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраняем промежуточные веса модели в определенное количество пройденных шагов (steps), cледим за уменьшением loss yа обучающем и валидационном датасетах:"
      ],
      "metadata": {
        "id": "odBHMccaLB-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-05-18T20:41:23.148Z"
        },
        "scrolled": true,
        "colab": {
          "referenced_widgets": [
            "d47fad5a6f304847b97cb6df1cf0c2d8",
            "1a1ffe60f5ba4b519a9256d132ba3041",
            "5359357e2f37487f89dfdb95d800f47b",
            "74547a7ff7774798b9bafd508f73c6ad",
            "045968f40bf749b592e48df6780f3a59",
            "cd3da4f062634882aa88f405ce2fe11a",
            "b8de23cf2233473097decc5863f33516",
            "d560f6e06c7643669dcc673288e80672"
          ]
        },
        "id": "ms05n5QiJ5VL",
        "outputId": "5d3e42a9-9194-4eba-8e88-ef9a60db09f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  train  1000 \n",
            "=====================\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005002498626708984,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d47fad5a6f304847b97cb6df1cf0c2d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 1000\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004996061325073242,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 611951,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a1ffe60f5ba4b519a9256d132ba3041",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/611951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, step 300/300: train loss: 3.2998  val loss: 4.1076\n",
            "epoch 0, step 600/600: train loss: 1.8564  val loss: 4.0339\n",
            "epoch 0, step 900/900: train loss: 1.3853  val loss: 4.1101\n",
            "1000 1000\n",
            "\n",
            "\n",
            "\n",
            "  train  10000 \n",
            "=====================\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005999565124511719,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5359357e2f37487f89dfdb95d800f47b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 10000\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00600433349609375,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 611951,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74547a7ff7774798b9bafd508f73c6ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/611951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, step 300/300: train loss: 3.1746  val loss: 4.1643\n",
            "epoch 0, step 600/600: train loss: 1.8473  val loss: 4.0684\n",
            "epoch 0, step 900/900: train loss: 1.3142  val loss: 4.2081\n",
            "epoch 0, step 1200/1200: train loss: 1.0787  val loss: 4.2530\n",
            "epoch 0, step 1500/1500: train loss: 0.9359  val loss: 4.2127\n",
            "epoch 0, step 1800/1800: train loss: 0.7887  val loss: 4.1764\n",
            "epoch 0, step 2100/2100: train loss: 0.7222  val loss: 4.0566\n",
            "epoch 0, step 2400/2400: train loss: 0.6963  val loss: 4.0206\n",
            "epoch 0, step 2700/2700: train loss: 0.6862  val loss: 3.9439\n",
            "epoch 0, step 3000/3000: train loss: 0.6125  val loss: 3.8941\n",
            "epoch 0, step 3300/3300: train loss: 0.6165  val loss: 4.0259\n",
            "epoch 0, step 3600/3600: train loss: 0.6058  val loss: 3.8727\n",
            "epoch 0, step 3900/3900: train loss: 0.5494  val loss: 3.8483\n",
            "epoch 0, step 4200/4200: train loss: 0.4929  val loss: 3.7440\n",
            "epoch 0, step 4500/4500: train loss: 0.5050  val loss: 3.7352\n",
            "epoch 0, step 4800/4800: train loss: 0.4978  val loss: 3.5622\n",
            "epoch 0, step 5100/5100: train loss: 0.5016  val loss: 3.5409\n",
            "epoch 0, step 5400/5400: train loss: 0.4630  val loss: 3.5833\n",
            "epoch 0, step 5700/5700: train loss: 0.4676  val loss: 3.5144\n",
            "epoch 0, step 6000/6000: train loss: 0.4380  val loss: 3.3897\n",
            "epoch 0, step 6300/6300: train loss: 0.4569  val loss: 3.4133\n",
            "epoch 0, step 6600/6600: train loss: 0.4501  val loss: 3.3496\n",
            "epoch 0, step 6900/6900: train loss: 0.3967  val loss: 3.3300\n",
            "epoch 0, step 7200/7200: train loss: 0.4219  val loss: 3.1846\n",
            "epoch 0, step 7500/7500: train loss: 0.4079  val loss: 3.1797\n",
            "epoch 0, step 7800/7800: train loss: 0.4039  val loss: 3.2399\n",
            "epoch 0, step 8100/8100: train loss: 0.3968  val loss: 3.2472\n",
            "epoch 0, step 8400/8400: train loss: 0.4175  val loss: 3.1401\n",
            "epoch 0, step 8700/8700: train loss: 0.3572  val loss: 3.1386\n",
            "epoch 0, step 9000/9000: train loss: 0.3574  val loss: 3.0744\n",
            "epoch 0, step 9300/9300: train loss: 0.3940  val loss: 3.1560\n",
            "epoch 0, step 9600/9600: train loss: 0.3795  val loss: 3.1370\n",
            "epoch 0, step 9900/9900: train loss: 0.3791  val loss: 2.9850\n",
            "10000 10000\n",
            "\n",
            "\n",
            "\n",
            "  train  50000 \n",
            "=====================\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.006001472473144531,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "045968f40bf749b592e48df6780f3a59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 50000\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.007999658584594727,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 611951,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd3da4f062634882aa88f405ce2fe11a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/611951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, step 300/300: train loss: 3.2860  val loss: 4.1301\n",
            "epoch 0, step 600/600: train loss: 1.8862  val loss: 4.1664\n",
            "epoch 0, step 900/900: train loss: 1.3979  val loss: 4.2682\n",
            "epoch 0, step 1200/1200: train loss: 1.0555  val loss: 4.3132\n",
            "epoch 0, step 1500/1500: train loss: 0.8961  val loss: 4.2703\n",
            "epoch 0, step 1800/1800: train loss: 0.8397  val loss: 4.2189\n",
            "epoch 0, step 2100/2100: train loss: 0.7848  val loss: 4.2034\n",
            "epoch 0, step 2400/2400: train loss: 0.7126  val loss: 4.1734\n",
            "epoch 0, step 2700/2700: train loss: 0.6540  val loss: 4.2152\n",
            "epoch 0, step 3000/3000: train loss: 0.6020  val loss: 3.9939\n",
            "epoch 0, step 3300/3300: train loss: 0.6346  val loss: 3.9203\n",
            "epoch 0, step 3600/3600: train loss: 0.5841  val loss: 3.8659\n",
            "epoch 0, step 3900/3900: train loss: 0.5627  val loss: 3.8412\n",
            "epoch 0, step 4200/4200: train loss: 0.5454  val loss: 3.6696\n",
            "epoch 0, step 4500/4500: train loss: 0.4919  val loss: 3.7257\n",
            "epoch 0, step 4800/4800: train loss: 0.5402  val loss: 3.7493\n",
            "epoch 0, step 5100/5100: train loss: 0.4925  val loss: 3.6646\n",
            "epoch 0, step 5400/5400: train loss: 0.4693  val loss: 3.6402\n",
            "epoch 0, step 5700/5700: train loss: 0.4823  val loss: 3.6644\n",
            "epoch 0, step 6000/6000: train loss: 0.4751  val loss: 3.5497\n",
            "epoch 0, step 6300/6300: train loss: 0.4788  val loss: 3.4589\n",
            "epoch 0, step 6600/6600: train loss: 0.4281  val loss: 3.4825\n",
            "epoch 0, step 6900/6900: train loss: 0.4348  val loss: 3.5206\n",
            "epoch 0, step 7200/7200: train loss: 0.4191  val loss: 3.5284\n",
            "epoch 0, step 7500/7500: train loss: 0.4157  val loss: 3.2792\n",
            "epoch 0, step 7800/7800: train loss: 0.3893  val loss: 3.3411\n",
            "epoch 0, step 8100/8100: train loss: 0.4193  val loss: 3.3172\n",
            "epoch 0, step 8400/8400: train loss: 0.4065  val loss: 3.2840\n",
            "epoch 0, step 8700/8700: train loss: 0.4115  val loss: 3.3277\n",
            "epoch 0, step 9000/9000: train loss: 0.3965  val loss: 3.2351\n",
            "epoch 0, step 9300/9300: train loss: 0.3734  val loss: 3.2402\n",
            "epoch 0, step 9600/9600: train loss: 0.3833  val loss: 3.2211\n",
            "epoch 0, step 9900/9900: train loss: 0.3640  val loss: 3.0884\n",
            "epoch 0, step 10200/10200: train loss: 0.3482  val loss: 3.1300\n",
            "epoch 0, step 10500/10500: train loss: 0.3735  val loss: 3.1580\n",
            "epoch 0, step 10800/10800: train loss: 0.3501  val loss: 3.0377\n",
            "epoch 0, step 11100/11100: train loss: 0.3459  val loss: 3.0721\n",
            "epoch 0, step 11400/11400: train loss: 0.3499  val loss: 3.0986\n",
            "epoch 0, step 11700/11700: train loss: 0.3693  val loss: 3.0382\n",
            "epoch 0, step 12000/12000: train loss: 0.3753  val loss: 3.0066\n",
            "epoch 0, step 12300/12300: train loss: 0.3448  val loss: 2.9796\n",
            "epoch 0, step 12600/12600: train loss: 0.3270  val loss: 3.0512\n",
            "epoch 0, step 12900/12900: train loss: 0.3208  val loss: 3.0542\n",
            "epoch 0, step 13200/13200: train loss: 0.3062  val loss: 3.1255\n",
            "epoch 0, step 13500/13500: train loss: 0.3253  val loss: 2.9791\n",
            "epoch 0, step 13800/13800: train loss: 0.3181  val loss: 2.9945\n",
            "epoch 0, step 14100/14100: train loss: 0.3260  val loss: 2.9260\n",
            "epoch 0, step 14400/14400: train loss: 0.3075  val loss: 3.0588\n",
            "epoch 0, step 14700/14700: train loss: 0.3073  val loss: 3.1325\n",
            "epoch 0, step 15000/15000: train loss: 0.2999  val loss: 2.8495\n",
            "epoch 0, step 15300/15300: train loss: 0.3175  val loss: 2.8155\n",
            "epoch 0, step 15600/15600: train loss: 0.3191  val loss: 2.8188\n",
            "epoch 0, step 15900/15900: train loss: 0.3005  val loss: 2.7771\n",
            "epoch 0, step 16200/16200: train loss: 0.2872  val loss: 2.8108\n",
            "epoch 0, step 16500/16500: train loss: 0.3013  val loss: 2.8500\n",
            "epoch 0, step 16800/16800: train loss: 0.2993  val loss: 2.8164\n",
            "epoch 0, step 17100/17100: train loss: 0.3190  val loss: 2.7050\n",
            "epoch 0, step 17400/17400: train loss: 0.3207  val loss: 2.7685\n",
            "epoch 0, step 17700/17700: train loss: 0.2943  val loss: 2.7707\n",
            "epoch 0, step 18000/18000: train loss: 0.2856  val loss: 2.7626\n",
            "epoch 0, step 18300/18300: train loss: 0.3022  val loss: 2.6781\n",
            "epoch 0, step 18600/18600: train loss: 0.2976  val loss: 2.8057\n",
            "epoch 0, step 18900/18900: train loss: 0.2651  val loss: 2.8123\n",
            "epoch 0, step 19200/19200: train loss: 0.2663  val loss: 2.9535\n",
            "epoch 0, step 19500/19500: train loss: 0.2766  val loss: 2.8456\n",
            "epoch 0, step 19800/19800: train loss: 0.2680  val loss: 2.8852\n",
            "epoch 0, step 20100/20100: train loss: 0.2651  val loss: 2.6600\n",
            "epoch 0, step 20400/20400: train loss: 0.2436  val loss: 2.7010\n",
            "epoch 0, step 20700/20700: train loss: 0.2658  val loss: 2.7194\n",
            "epoch 0, step 21000/21000: train loss: 0.2788  val loss: 2.5337\n",
            "epoch 0, step 21300/21300: train loss: 0.2700  val loss: 2.6349\n",
            "epoch 0, step 21600/21600: train loss: 0.2670  val loss: 2.7034\n",
            "epoch 0, step 21900/21900: train loss: 0.2480  val loss: 2.5347\n",
            "epoch 0, step 22200/22200: train loss: 0.2595  val loss: 2.5828\n",
            "epoch 0, step 22500/22500: train loss: 0.2490  val loss: 2.5639\n",
            "epoch 0, step 22800/22800: train loss: 0.2589  val loss: 2.4767\n",
            "epoch 0, step 23100/23100: train loss: 0.2581  val loss: 2.5823\n",
            "epoch 0, step 23400/23400: train loss: 0.2755  val loss: 2.6264\n",
            "epoch 0, step 23700/23700: train loss: 0.2445  val loss: 2.5279\n",
            "epoch 0, step 24000/24000: train loss: 0.2607  val loss: 2.6664\n",
            "epoch 0, step 24300/24300: train loss: 0.2679  val loss: 2.6293\n",
            "epoch 0, step 24600/24600: train loss: 0.2707  val loss: 2.5870\n",
            "epoch 0, step 24900/24900: train loss: 0.2365  val loss: 2.6447\n",
            "epoch 0, step 25200/25200: train loss: 0.2613  val loss: 2.6236\n",
            "epoch 0, step 25500/25500: train loss: 0.2348  val loss: 2.6130\n",
            "epoch 0, step 25800/25800: train loss: 0.2505  val loss: 2.6475\n",
            "epoch 0, step 26100/26100: train loss: 0.2546  val loss: 2.5528\n",
            "epoch 0, step 26400/26400: train loss: 0.2577  val loss: 2.7179\n",
            "epoch 0, step 26700/26700: train loss: 0.2580  val loss: 2.7191\n",
            "epoch 0, step 27000/27000: train loss: 0.2569  val loss: 2.7392\n",
            "epoch 0, step 27300/27300: train loss: 0.2291  val loss: 2.6126\n",
            "epoch 0, step 27600/27600: train loss: 0.2335  val loss: 2.5190\n",
            "epoch 0, step 27900/27900: train loss: 0.2243  val loss: 2.4859\n",
            "epoch 0, step 28200/28200: train loss: 0.2432  val loss: 2.5702\n",
            "epoch 0, step 28500/28500: train loss: 0.2582  val loss: 2.5024\n",
            "epoch 0, step 28800/28800: train loss: 0.2528  val loss: 2.4527\n",
            "epoch 0, step 29100/29100: train loss: 0.2243  val loss: 2.3648\n",
            "epoch 0, step 29400/29400: train loss: 0.2290  val loss: 2.1717\n",
            "epoch 0, step 29700/29700: train loss: 0.2260  val loss: 2.2144\n",
            "epoch 0, step 30000/30000: train loss: 0.2235  val loss: 2.1238\n",
            "epoch 0, step 30300/30300: train loss: 0.2423  val loss: 2.2132\n",
            "epoch 0, step 30600/30600: train loss: 0.2174  val loss: 2.1415\n",
            "epoch 0, step 30900/30900: train loss: 0.2153  val loss: 2.2105\n",
            "epoch 0, step 31200/31200: train loss: 0.2015  val loss: 2.2652\n",
            "epoch 0, step 31500/31500: train loss: 0.2410  val loss: 2.2816\n",
            "epoch 0, step 31800/31800: train loss: 0.2225  val loss: 2.2847\n",
            "epoch 0, step 32100/32100: train loss: 0.2204  val loss: 2.2662\n",
            "epoch 0, step 32400/32400: train loss: 0.1935  val loss: 2.4398\n",
            "epoch 0, step 32700/32700: train loss: 0.2335  val loss: 2.3291\n",
            "epoch 0, step 33000/33000: train loss: 0.2432  val loss: 2.3147\n",
            "epoch 0, step 33300/33300: train loss: 0.2223  val loss: 2.2992\n",
            "epoch 0, step 33600/33600: train loss: 0.2202  val loss: 2.3564\n",
            "epoch 0, step 33900/33900: train loss: 0.2333  val loss: 2.3724\n",
            "epoch 0, step 34200/34200: train loss: 0.2404  val loss: 2.3035\n",
            "epoch 0, step 34500/34500: train loss: 0.2142  val loss: 2.3120\n",
            "epoch 0, step 34800/34800: train loss: 0.2149  val loss: 2.1668\n",
            "epoch 0, step 35100/35100: train loss: 0.2484  val loss: 2.1668\n",
            "epoch 0, step 35400/35400: train loss: 0.2187  val loss: 2.2031\n",
            "epoch 0, step 35700/35700: train loss: 0.2293  val loss: 2.1518\n",
            "epoch 0, step 36000/36000: train loss: 0.2349  val loss: 2.1609\n",
            "epoch 0, step 36300/36300: train loss: 0.2206  val loss: 2.0795\n",
            "epoch 0, step 36600/36600: train loss: 0.2210  val loss: 2.2087\n",
            "epoch 0, step 36900/36900: train loss: 0.1989  val loss: 2.1749\n",
            "epoch 0, step 37200/37200: train loss: 0.2089  val loss: 2.1656\n",
            "epoch 0, step 37500/37500: train loss: 0.2003  val loss: 2.1963\n",
            "epoch 0, step 37800/37800: train loss: 0.2139  val loss: 2.2531\n",
            "epoch 0, step 38100/38100: train loss: 0.1972  val loss: 2.1839\n",
            "epoch 0, step 38400/38400: train loss: 0.2095  val loss: 2.1759\n",
            "epoch 0, step 38700/38700: train loss: 0.2137  val loss: 2.1355\n",
            "epoch 0, step 39000/39000: train loss: 0.2090  val loss: 2.1540\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, step 39300/39300: train loss: 0.2167  val loss: 2.0731\n",
            "epoch 0, step 39600/39600: train loss: 0.2218  val loss: 1.9828\n",
            "epoch 0, step 39900/39900: train loss: 0.1817  val loss: 2.0424\n",
            "epoch 0, step 40200/40200: train loss: 0.1768  val loss: 2.0081\n",
            "epoch 0, step 40500/40500: train loss: 0.2090  val loss: 2.0733\n",
            "epoch 0, step 40800/40800: train loss: 0.1971  val loss: 2.0707\n",
            "epoch 0, step 41100/41100: train loss: 0.1910  val loss: 2.1339\n",
            "epoch 0, step 41400/41400: train loss: 0.2221  val loss: 2.1955\n",
            "epoch 0, step 41700/41700: train loss: 0.1972  val loss: 2.2512\n",
            "epoch 0, step 42000/42000: train loss: 0.1993  val loss: 2.1843\n",
            "epoch 0, step 42300/42300: train loss: 0.1922  val loss: 2.2294\n",
            "epoch 0, step 42600/42600: train loss: 0.1761  val loss: 2.2913\n",
            "epoch 0, step 42900/42900: train loss: 0.2075  val loss: 2.2996\n",
            "epoch 0, step 43200/43200: train loss: 0.1973  val loss: 2.2675\n",
            "epoch 0, step 43500/43500: train loss: 0.1751  val loss: 2.2776\n",
            "epoch 0, step 43800/43800: train loss: 0.1963  val loss: 2.4703\n",
            "epoch 0, step 44100/44100: train loss: 0.1901  val loss: 2.4786\n",
            "epoch 0, step 44400/44400: train loss: 0.2199  val loss: 2.3462\n",
            "epoch 0, step 44700/44700: train loss: 0.1869  val loss: 2.4303\n",
            "epoch 0, step 45000/45000: train loss: 0.1688  val loss: 2.4066\n",
            "epoch 0, step 45300/45300: train loss: 0.2040  val loss: 2.4245\n",
            "epoch 0, step 45600/45600: train loss: 0.1908  val loss: 2.3122\n",
            "epoch 0, step 45900/45900: train loss: 0.1932  val loss: 2.4493\n",
            "epoch 0, step 46200/46200: train loss: 0.1784  val loss: 2.3097\n",
            "epoch 0, step 46500/46500: train loss: 0.1900  val loss: 2.3393\n",
            "epoch 0, step 46800/46800: train loss: 0.1972  val loss: 2.5277\n",
            "epoch 0, step 47100/47100: train loss: 0.1833  val loss: 2.4788\n",
            "epoch 0, step 47400/47400: train loss: 0.1937  val loss: 2.4058\n",
            "epoch 0, step 47700/47700: train loss: 0.1636  val loss: 2.1886\n",
            "epoch 0, step 48000/48000: train loss: 0.1874  val loss: 2.1400\n",
            "epoch 0, step 48300/48300: train loss: 0.1849  val loss: 2.1115\n",
            "epoch 0, step 48600/48600: train loss: 0.1971  val loss: 2.1908\n",
            "epoch 0, step 48900/48900: train loss: 0.1862  val loss: 2.3394\n",
            "epoch 0, step 49200/49200: train loss: 0.1747  val loss: 2.3955\n",
            "epoch 0, step 49500/49500: train loss: 0.2090  val loss: 2.3291\n",
            "epoch 0, step 49800/49800: train loss: 0.1784  val loss: 2.2277\n",
            "50000 50000\n",
            "\n",
            "\n",
            "\n",
            "  train  100000 \n",
            "=====================\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.014010906219482422,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 1000,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8de23cf2233473097decc5863f33516",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 100000\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005358219146728516,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 611951,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d560f6e06c7643669dcc673288e80672",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/611951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, step 300/300: train loss: 3.2536  val loss: 4.0554\n",
            "epoch 0, step 600/600: train loss: 1.8092  val loss: 4.1634\n",
            "epoch 0, step 900/900: train loss: 1.3685  val loss: 4.3373\n",
            "epoch 0, step 1200/1200: train loss: 1.0699  val loss: 4.4141\n",
            "epoch 0, step 1500/1500: train loss: 0.9671  val loss: 4.2847\n",
            "epoch 0, step 1800/1800: train loss: 0.8041  val loss: 4.1765\n",
            "epoch 0, step 2100/2100: train loss: 0.7396  val loss: 3.9864\n",
            "epoch 0, step 2400/2400: train loss: 0.6766  val loss: 3.8859\n",
            "epoch 0, step 2700/2700: train loss: 0.6847  val loss: 3.8507\n",
            "epoch 0, step 3000/3000: train loss: 0.6101  val loss: 3.7432\n",
            "epoch 0, step 3300/3300: train loss: 0.6280  val loss: 3.6475\n",
            "epoch 0, step 3600/3600: train loss: 0.5601  val loss: 3.4806\n",
            "epoch 0, step 3900/3900: train loss: 0.5598  val loss: 3.4594\n",
            "epoch 0, step 4200/4200: train loss: 0.5082  val loss: 3.4106\n",
            "epoch 0, step 4500/4500: train loss: 0.5514  val loss: 3.5187\n",
            "epoch 0, step 4800/4800: train loss: 0.4928  val loss: 3.2502\n",
            "epoch 0, step 5100/5100: train loss: 0.4753  val loss: 3.2407\n",
            "epoch 0, step 5400/5400: train loss: 0.4482  val loss: 3.3801\n",
            "epoch 0, step 5700/5700: train loss: 0.4580  val loss: 3.4083\n",
            "epoch 0, step 6000/6000: train loss: 0.4295  val loss: 3.2722\n",
            "epoch 0, step 6300/6300: train loss: 0.4468  val loss: 3.2404\n",
            "epoch 0, step 6600/6600: train loss: 0.4606  val loss: 3.1916\n",
            "epoch 0, step 6900/6900: train loss: 0.4655  val loss: 3.2428\n",
            "epoch 0, step 7200/7200: train loss: 0.4398  val loss: 3.3980\n",
            "epoch 0, step 7500/7500: train loss: 0.4167  val loss: 3.3847\n",
            "epoch 0, step 7800/7800: train loss: 0.4116  val loss: 3.2678\n",
            "epoch 0, step 8100/8100: train loss: 0.3943  val loss: 3.0777\n",
            "epoch 0, step 8400/8400: train loss: 0.4018  val loss: 2.9920\n",
            "epoch 0, step 8700/8700: train loss: 0.3941  val loss: 3.0751\n",
            "epoch 0, step 9000/9000: train loss: 0.3799  val loss: 3.0032\n",
            "epoch 0, step 9300/9300: train loss: 0.3827  val loss: 3.0561\n",
            "epoch 0, step 9600/9600: train loss: 0.3605  val loss: 2.9651\n",
            "epoch 0, step 9900/9900: train loss: 0.3815  val loss: 2.9613\n",
            "epoch 0, step 10200/10200: train loss: 0.3552  val loss: 2.8668\n",
            "epoch 0, step 10500/10500: train loss: 0.3474  val loss: 2.7950\n",
            "epoch 0, step 10800/10800: train loss: 0.3584  val loss: 2.7292\n",
            "epoch 0, step 11100/11100: train loss: 0.3554  val loss: 2.7623\n",
            "epoch 0, step 11400/11400: train loss: 0.3607  val loss: 2.9208\n",
            "epoch 0, step 11700/11700: train loss: 0.3310  val loss: 3.0215\n",
            "epoch 0, step 12000/12000: train loss: 0.3550  val loss: 2.8963\n",
            "epoch 0, step 12300/12300: train loss: 0.3283  val loss: 2.8343\n",
            "epoch 0, step 12600/12600: train loss: 0.3471  val loss: 2.8823\n",
            "epoch 0, step 12900/12900: train loss: 0.3334  val loss: 2.9436\n",
            "epoch 0, step 13200/13200: train loss: 0.3361  val loss: 2.8494\n",
            "epoch 0, step 13500/13500: train loss: 0.3337  val loss: 2.7496\n",
            "epoch 0, step 13800/13800: train loss: 0.3429  val loss: 2.7793\n",
            "epoch 0, step 14100/14100: train loss: 0.3020  val loss: 2.7945\n",
            "epoch 0, step 14400/14400: train loss: 0.3258  val loss: 2.7230\n",
            "epoch 0, step 14700/14700: train loss: 0.3139  val loss: 2.7228\n",
            "epoch 0, step 15000/15000: train loss: 0.3116  val loss: 2.7311\n",
            "epoch 0, step 15300/15300: train loss: 0.3290  val loss: 2.6564\n",
            "epoch 0, step 15600/15600: train loss: 0.3402  val loss: 2.7392\n",
            "epoch 0, step 15900/15900: train loss: 0.2865  val loss: 2.7014\n",
            "epoch 0, step 16200/16200: train loss: 0.3085  val loss: 2.7014\n",
            "epoch 0, step 16500/16500: train loss: 0.2672  val loss: 2.6379\n",
            "epoch 0, step 16800/16800: train loss: 0.2993  val loss: 2.7316\n",
            "epoch 0, step 17100/17100: train loss: 0.2930  val loss: 2.7427\n",
            "epoch 0, step 17400/17400: train loss: 0.2928  val loss: 2.7387\n",
            "epoch 0, step 17700/17700: train loss: 0.2951  val loss: 2.7747\n",
            "epoch 0, step 18000/18000: train loss: 0.3020  val loss: 2.7296\n",
            "epoch 0, step 18300/18300: train loss: 0.3049  val loss: 2.7356\n",
            "epoch 0, step 18600/18600: train loss: 0.2930  val loss: 2.6363\n",
            "epoch 0, step 18900/18900: train loss: 0.2994  val loss: 2.5949\n",
            "epoch 0, step 19200/19200: train loss: 0.2888  val loss: 2.7813\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for steps in [1000, 10000, 50000, 100000, 500000, 1000000, 1359892]:\n",
        "    for dname, d in datasets.items():\n",
        "        print(f'\\n\\n\\n  {dname}  {steps} \\n=====================\\n\\n')\n",
        "        model = train_model(\n",
        "            d['form'].tolist(), \n",
        "            d['data'].tolist(), \n",
        "            model_name=model_name, \n",
        "            batch_size=2, \n",
        "            max_epochs=1000, \n",
        "            max_steps=steps\n",
        "        )\n",
        "        model.save_pretrained(f't5_small_{dname}_{steps}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTu3zq0XJ5VN"
      },
      "source": [
        "## RuPrompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:29:40.124853Z",
          "start_time": "2023-05-26T20:29:40.109854Z"
        },
        "id": "jL8RrMWaJ5VN"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем уже предобученную модель _rugpt3large_based_on_gpt2_:"
      ],
      "metadata": {
        "id": "17gKK0PELJf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:30:01.571366Z",
          "start_time": "2023-05-26T20:29:46.115668Z"
        },
        "id": "EpPQrylWJ5VN",
        "outputId": "456a400a-497e-42cc-a806-63b81dfcb1f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "backbone_id = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(backbone_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(backbone_id, pad_token=\"<pad>\", eos_token=\"<pad>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:30:13.199433Z",
          "start_time": "2023-05-26T20:30:13.121433Z"
        },
        "id": "cjUCKK0LJ5VN"
      },
      "outputs": [],
      "source": [
        "from ruprompts import PromptFormat\n",
        "\n",
        "prompt_format = PromptFormat(\"<P*100>{cluster}<P*20>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkgmQlMEJ5VN"
      },
      "source": [
        "Определяем параметризацию обучаемых вложений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:30:16.138959Z",
          "start_time": "2023-05-26T20:30:16.124959Z"
        },
        "id": "_u9HwGuHJ5VN"
      },
      "outputs": [],
      "source": [
        "from ruprompts import TensorPromptProvider\n",
        "from transformers import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "prompt_provider = TensorPromptProvider()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQu0JmJJ5VO"
      },
      "source": [
        "Собираем формат prompt и провайдер prompt в объект prompt и применяем его к модели и токенизатору, т.е. добавляем в токенизатор специальные токены и модифицируем слой входных вложений модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:30:18.345494Z",
          "start_time": "2023-05-26T20:30:18.274488Z"
        },
        "id": "Nlx3JhcnJ5VO"
      },
      "outputs": [],
      "source": [
        "from ruprompts import Prompt\n",
        "\n",
        "prompt = Prompt(prompt_format, prompt_provider)\n",
        "prompt.patch(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3taWUOxCJ5VO"
      },
      "source": [
        "Начинаем предпроцессинг данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-26T20:32:29.097011Z",
          "start_time": "2023-05-26T20:32:28.112924Z"
        },
        "colab": {
          "referenced_widgets": [
            "36e211c8357042ac8233292e0d7efe6a",
            "35e8079f53e042129823e60e2e5e1613",
            "",
            "1bc815ee7fa64982aa07515fdf5ff3f8"
          ]
        },
        "id": "zj1tZjwAJ5VO",
        "outputId": "b9924565-1ab1-4680-ac14-374265c591ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/user/.cache/huggingface/datasets/csv/default-3cc147501614d040/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0070002079010009766,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "Downloading data files",
              "rate": null,
              "total": 2,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e211c8357042ac8233292e0d7efe6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004999876022338867,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "Extracting data files",
              "rate": null,
              "total": 2,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35e8079f53e042129823e60e2e5e1613",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.006000518798828125,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "Generating train split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004998922348022461,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "Generating val split",
              "rate": null,
              "total": 0,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating val split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/user/.cache/huggingface/datasets/csv/default-3cc147501614d040/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004998922348022461,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 2,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bc815ee7fa64982aa07515fdf5ff3f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "datasets = load_dataset('csv', data_files={'train': './data/clustered_data/taiga_train_data_dbscan__clustered.csv', 'val': './data/clustered_data/taiga_dev_data_dbscan__clustered.csv'})\n",
        "train_dataset = datasets[\"train\"]\n",
        "val_dataset = datasets[\"val\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-05-26T20:32:48.700Z"
        },
        "colab": {
          "referenced_widgets": [
            "0e34ef384bf94903bb087f4a05e50f19"
          ]
        },
        "id": "D60mgzfPJ5VP",
        "outputId": "a3a98db0-500b-4b5a-8aea-4c8ee603277e"
      },
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005998849868774414,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 29,
              "postfix": null,
              "prefix": "Map",
              "rate": null,
              "total": 74900,
              "unit": " examples",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e34ef384bf94903bb087f4a05e50f19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/74900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ruprompts import Text2TextPreprocessor\n",
        "\n",
        "preprocessor = Text2TextPreprocessor(\n",
        "    prompt_format=prompt_format,\n",
        "    tokenizer=tokenizer,\n",
        "    target_field=\"data\",\n",
        "    max_tokens=1792,\n",
        "    truncation_field=\"cluster\",\n",
        ")\n",
        "\n",
        "train_dataset = train_dataset.map(preprocessor)\n",
        "valid_dataset = val_dataset.map(preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем параметры обучения, выбираем размер батча, количество степов, функции потерь и т.д.:"
      ],
      "metadata": {
        "id": "kJjvvJewLi51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-05-24T07:48:10.854Z"
        },
        "id": "OBeHv_ehJ5VP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./try23_05_taiga_dbscan\",\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=5,\n",
        "    gradient_accumulation_steps=1,\n",
        "    eval_steps=1000, #100\n",
        "    save_steps=1000, #100\n",
        "    logging_steps=1000, #100\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    learning_rate=0.1,\n",
        "    max_steps=15000, #100000\n",
        "    report_to=\"tensorboard\",\n",
        "    # report_to=[\"tensorboard\", \"wandb\"],  # uncomment to log to WandB\n",
        "    logging_dir=\"logs\",\n",
        "    seed=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVtAuPaOJ5VP"
      },
      "source": [
        "Выбираем параметры оптимизации модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-05-24T07:48:11.793Z"
        },
        "id": "-osTo4rMJ5VQ"
      },
      "outputs": [],
      "source": [
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(prompt_provider.parameters(), lr=training_args.learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=2000,\n",
        "    num_training_steps=training_args.max_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02oyEdcrJ5VQ"
      },
      "source": [
        "Начинаем обучение модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-05-24T07:48:13.272Z"
        },
        "scrolled": true,
        "id": "-iJePP7AJ5VQ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from ruprompts.callbacks import (\n",
        "    FreezeTransformerUnfreezePrompt,\n",
        "    ReduceCheckpoint,\n",
        "    SavePretrainedPrompt,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=preprocessor.collate_fn(),\n",
        "    optimizers=(optimizer, scheduler),\n",
        "    callbacks=[FreezeTransformerUnfreezePrompt(), ReduceCheckpoint(), SavePretrainedPrompt(prompt)],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}