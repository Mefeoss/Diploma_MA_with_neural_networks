{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9AsEN4VJ5U8"
   },
   "source": [
    "# Обучение нейронных сетей\n",
    "### Автор: _Феоктистова Эмма Александровна, 4 курс ФиКЛ_\n",
    "### Научный руководитель: _Проф. Ляшевская О. Н._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsQmjGdJ5U-"
   },
   "source": [
    "## 0. Необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:34.416653Z",
     "start_time": "2023-05-29T08:30:34.392650Z"
    },
    "id": "s8lIRGoLJ5U-"
   },
   "outputs": [],
   "source": [
    "import conllu\n",
    "from conllu import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:34.590650Z",
     "start_time": "2023-05-29T08:30:34.577654Z"
    },
    "id": "XglYGT18J5U_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:38.648779Z",
     "start_time": "2023-05-29T08:30:36.418419Z"
    },
    "id": "A0tSL9tCJ5U_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:42.743506Z",
     "start_time": "2023-05-29T08:30:38.809780Z"
    },
    "id": "ce1ZQtFAJ5U_"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:50.366247Z",
     "start_time": "2023-05-29T08:30:42.906508Z"
    },
    "id": "Cvw1GR8jJ5U_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:30:50.541246Z",
     "start_time": "2023-05-29T08:30:50.529247Z"
    },
    "id": "wxMSiyXfJ5VA"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPE4wctUJ5VI"
   },
   "source": [
    "## 2. T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:00.326400Z",
     "start_time": "2023-05-29T08:30:50.721248Z"
    },
    "id": "_QrsrQbRJ5VJ",
    "outputId": "cba1c418-b6ae-4abc-b304-3ebeb9def08a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[torch]==4.3\n",
      "  Using cached transformers-4.3.0-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (4.64.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (0.0.53)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (2022.4.24)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (21.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers[torch]==4.3) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from torch>=1.0->transformers[torch]==4.3) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from tqdm>=4.27->transformers[torch]==4.3) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from packaging->transformers[torch]==4.3) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers[torch]==4.3) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers[torch]==4.3) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers[torch]==4.3) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers[torch]==4.3) (1.26.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from sacremoses->transformers[torch]==4.3) (8.1.3)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed tokenizers-0.10.3 transformers-4.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ruprompts 0.1.4 requires transformers<5.0.0,>=4.6.0, but you have transformers 4.3.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.udpipe in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (1.3.0.1)\n",
      "Requirement already satisfied: ruprompts in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (0.1.4)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from ruprompts) (4.2.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.13.3 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from ruprompts) (2.13.3)\n",
      "Requirement already satisfied: torchtyping<0.2.0,>=0.1.4 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from ruprompts) (0.1.4)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.10.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from ruprompts) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (2022.4.24)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->ruprompts) (21.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<5.0.0,>=4.6.0->ruprompts) (2023.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->ruprompts) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from tqdm>=4.27->transformers<5.0.0,>=4.6.0->ruprompts) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->ruprompts) (1.26.9)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.3.0\n",
      "    Uninstalling transformers-4.3.0:\n",
      "      Successfully uninstalled transformers-4.3.0\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.conda\\envs\\clust\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]==4.3\n",
    "!pip install ufal.udpipe\n",
    "!pip install ruprompts\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pop43nH4KXjZ"
   },
   "source": [
    "Указываем путь к используемым датасетам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:02.700831Z",
     "start_time": "2023-05-29T08:32:00.488400Z"
    },
    "id": "ZWSXggG6J5VJ",
    "outputId": "59381514-fa9e-48ff-ecba-11d3fa75ee99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Анкета</td>\n",
       "      <td>анкета,NOUN,Inan,Nom,Fem,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>.,PUNCT,None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Начальник</td>\n",
       "      <td>начальник,NOUN,Anim,Nom,Masc,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>областного</td>\n",
       "      <td>областной,ADJ,Gen,Pos,Neut,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>управления</td>\n",
       "      <td>управление,NOUN,Inan,Gen,Neut,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359887</th>\n",
       "      <td>внимание</td>\n",
       "      <td>внимание,NOUN,Inan,Nom,Neut,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359888</th>\n",
       "      <td>-</td>\n",
       "      <td>-,PUNCT,None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359889</th>\n",
       "      <td>большая</td>\n",
       "      <td>большой,ADJ,Nom,Pos,Fem,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359890</th>\n",
       "      <td>редкость</td>\n",
       "      <td>редкость,NOUN,Inan,Nom,Fem,Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359891</th>\n",
       "      <td>.</td>\n",
       "      <td>.,PUNCT,None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1359892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               form                                data\n",
       "0            Анкета       анкета,NOUN,Inan,Nom,Fem,Sing\n",
       "1                 .                        .,PUNCT,None\n",
       "2         Начальник   начальник,NOUN,Anim,Nom,Masc,Sing\n",
       "3        областного     областной,ADJ,Gen,Pos,Neut,Sing\n",
       "4        управления  управление,NOUN,Inan,Gen,Neut,Sing\n",
       "...             ...                                 ...\n",
       "1359887    внимание    внимание,NOUN,Inan,Nom,Neut,Sing\n",
       "1359888           -                        -,PUNCT,None\n",
       "1359889     большая        большой,ADJ,Nom,Pos,Fem,Sing\n",
       "1359890    редкость     редкость,NOUN,Inan,Nom,Fem,Sing\n",
       "1359891           .                        .,PUNCT,None\n",
       "\n",
       "[1359892 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/prepared_data/SynTagRus/syntagrus_train_data.csv')\n",
    "val_data = pd.read_csv('data/prepared_data/SynTagRus/syntagrus_dev_data.csv')\n",
    "full_train_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "full_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:03.050834Z",
     "start_time": "2023-05-29T08:32:03.022834Z"
    },
    "id": "2jK8MF3RJ5VJ"
   },
   "outputs": [],
   "source": [
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        x - dict; example (from toxic comment):\n",
    "            {\n",
    "                'input_ids': [[55, 27, 103, 172], [157, 24529, 4088, 2], ...],\n",
    "                'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], ...]\n",
    "            }\n",
    "        y - the same dict as x (from neutral comment)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx - index of current object\n",
    "        \n",
    "        returns dict:\n",
    "            {\n",
    "                'input_ids': [157, 24529, 4088, 2] # 'input_ids' from `x` for `idx\n",
    "                'attention_mask': [1, 1, 1, 1] # 'attention_mask' from `x` for `idx`\n",
    "                'decoder_attention_mask': [1, 1, 1, 1] # 'attention_mask' from `y` for `idx`\n",
    "                'labels': [422, 584, 17940, 246] # 'input_ids' from `y` for `idx`\n",
    "            }\n",
    "        \"\"\"\n",
    "        assert idx < len(self.x['input_ids']) # idx must be less than len of 'toxic' list (list from column `toxic_comment`)\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n # * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:03.381522Z",
     "start_time": "2023-05-29T08:32:03.369523Z"
    },
    "id": "IMeSIjUOJ5VJ"
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"\n",
    "    A helpful function to clean all cached batches.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DykARe8_KsUJ"
   },
   "source": [
    "Выбираем модель (мы использовали более маленькую модель _T5-small_ для быстроты обучения): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:03.712523Z",
     "start_time": "2023-05-29T08:32:03.699523Z"
    },
    "id": "iImQMdW-J5VJ"
   },
   "outputs": [],
   "source": [
    "# model_name = 'sberbank-ai/ruT5-base'\n",
    "model_name = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:06.622391Z",
     "start_time": "2023-05-29T08:32:04.033524Z"
    },
    "id": "TOcbqw65J5VJ",
    "outputId": "fabba760-a7bd-4508-dac7-5d5fc25a6c5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvhEJxTHKwvw"
   },
   "source": [
    "Используем уже предобученный токенайзер и преобразуем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:07.475288Z",
     "start_time": "2023-05-29T08:32:06.943389Z"
    },
    "id": "Wh-ibyvzJ5VK"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:32:07.969288Z",
     "start_time": "2023-05-29T08:32:07.850290Z"
    },
    "id": "e4gi1LecJ5VK"
   },
   "outputs": [],
   "source": [
    "x, y = full_train_data['form'].tolist(), full_train_data['data'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:33:48.746113Z",
     "start_time": "2023-05-29T08:32:17.720293Z"
    },
    "id": "BDqMf2BxJ5VK"
   },
   "outputs": [],
   "source": [
    "dataset = PairsDataset(tokenizer(x), tokenizer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:33:58.469888Z",
     "start_time": "2023-05-29T08:33:58.444897Z"
    },
    "id": "2j-q90FBJ5VK",
    "outputId": "29ca856e-92a0-41b1-c7b3-38b183a7c005",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 2, 7184, 6652, 1757, 15517, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
       " 'decoder_attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3,\n",
       "  25873,\n",
       "  6652,\n",
       "  1757,\n",
       "  15517,\n",
       "  6,\n",
       "  7400,\n",
       "  7443,\n",
       "  6,\n",
       "  1570,\n",
       "  152,\n",
       "  6,\n",
       "  4168,\n",
       "  51,\n",
       "  6,\n",
       "  371,\n",
       "  15,\n",
       "  51,\n",
       "  6,\n",
       "  134,\n",
       "  53,\n",
       "  1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7H4PxuEK1a-"
   },
   "source": [
    "Определяем параметры обучающего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:34:08.512904Z",
     "start_time": "2023-05-29T08:34:08.501904Z"
    },
    "id": "IMi7npMlJ5VK"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    drop_last=False, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:34:25.531203Z",
     "start_time": "2023-05-29T08:34:18.267113Z"
    },
    "id": "uPzFnJKFJ5VK"
   },
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDyjhyhUK5LE"
   },
   "source": [
    "Функции для оценки модели и ее обучения с настраиваемыми параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:34:35.148630Z",
     "start_time": "2023-05-29T08:34:35.137628Z"
    },
    "id": "DJ2X4tcPJ5VK"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:34:45.084124Z",
     "start_time": "2023-05-29T08:34:45.058124Z"
    },
    "id": "NxlSA_pWJ5VL"
   },
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=30, \n",
    "    max_steps=1_000, \n",
    "    lr=3e-5,\n",
    "    gradient_accumulation_steps=1, \n",
    "    cleanup_step=100,\n",
    "    report_step=300,\n",
    "    window=100,\n",
    "):\n",
    "    cleanup()\n",
    "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "\n",
    "    ewm_loss = 0\n",
    "    step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in trange(max_epochs):\n",
    "        print(step, max_steps)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        tq = tqdm(train_dataloader)\n",
    "        for i, batch in enumerate(tq):\n",
    "            try:\n",
    "                batch['labels'][batch['labels']==0] = -100\n",
    "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print('error on step', i, e)\n",
    "                loss = None\n",
    "                cleanup()\n",
    "                continue\n",
    "            if i and i % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                step += 1\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "\n",
    "            if i % cleanup_step == 0:\n",
    "                cleanup()\n",
    "\n",
    "            w = 1 / min(i+1, window)\n",
    "            ewm_loss = ewm_loss * (1-w) + loss.item() * w # for averaging loss values\n",
    "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
    "\n",
    "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
    "                model.eval()\n",
    "                eval_loss = evaluate_model(model, val_dataloader)\n",
    "                model.train()\n",
    "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                model.save_pretrained(f't5_small_{dname}_{steps}')\n",
    "        \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:34:54.945751Z",
     "start_time": "2023-05-29T08:34:54.934753Z"
    },
    "id": "xXKQQTmfJ5VL"
   },
   "outputs": [],
   "source": [
    "def train_model(x, y, model_name, test_size=0.1, batch_size=8, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
    "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:35:04.677933Z",
     "start_time": "2023-05-29T08:35:04.667932Z"
    },
    "id": "3nQHJBG6J5VL"
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': full_train_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeZS2r9tK-Em"
   },
   "source": [
    "Используем _DataCollatorWithPadding_ для приведения наших данных в единный формат (в задачах NLP входные данные  обычно представляют собой последовательности токенов разной длины, данный класс приводит их к одной длине с помощью паддингов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:35:14.523991Z",
     "start_time": "2023-05-29T08:35:14.497993Z"
    },
    "id": "wlZMaHgdJ5VL"
   },
   "outputs": [],
   "source": [
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        features example:\n",
    "            [    \n",
    "                {\"foo\": [1, 2, 3], \"bar\": torch.tensor([0.1, 0.2, 0.3])},\n",
    "                {\"foo\": [4, 5, 6], \"bar\": torch.tensor([0.4, 0.5, 0.6])},\n",
    "                {\"foo\": [7, 8, 9], \"bar\": torch.tensor([0.7, 0.8, 0.9])},\n",
    "            ]\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odBHMccaLB-f"
   },
   "source": [
    "Сохраняем промежуточные веса модели в определенное количество пройденных шагов (steps), cледим за уменьшением loss на обучающем и валидационном датасетах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:29:25.836670Z",
     "start_time": "2023-05-29T08:35:24.243530Z"
    },
    "colab": {
     "referenced_widgets": [
      "d47fad5a6f304847b97cb6df1cf0c2d8",
      "1a1ffe60f5ba4b519a9256d132ba3041",
      "5359357e2f37487f89dfdb95d800f47b",
      "74547a7ff7774798b9bafd508f73c6ad",
      "045968f40bf749b592e48df6780f3a59",
      "cd3da4f062634882aa88f405ce2fe11a",
      "b8de23cf2233473097decc5863f33516",
      "d560f6e06c7643669dcc673288e80672"
     ]
    },
    "id": "ms05n5QiJ5VL",
    "outputId": "5d3e42a9-9194-4eba-8e88-ef9a60db09f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  train  1000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013573884963989258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c4f5588383413e82ed094d950f5382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013000726699829102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 611951,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3b5ca8b5234b9a972a850a549356c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/611951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 300/300: train loss: 3.1543  val loss: 4.1220\n",
      "epoch 0, step 600/600: train loss: 1.7736  val loss: 4.1360\n",
      "epoch 0, step 900/900: train loss: 1.3103  val loss: 4.2545\n",
      "1000 1000\n",
      "\n",
      "\n",
      "\n",
      "  train  5000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011995792388916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf7dde788c946d09fa0f0688897bebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015998125076293945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 611951,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8470eadda2964e48a2a704ceed636b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/611951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 300/300: train loss: 3.2320  val loss: 4.1585\n",
      "epoch 0, step 600/600: train loss: 1.8079  val loss: 4.1765\n",
      "epoch 0, step 900/900: train loss: 1.3811  val loss: 4.3152\n",
      "epoch 0, step 1200/1200: train loss: 1.1132  val loss: 4.1888\n",
      "epoch 0, step 1500/1500: train loss: 0.9747  val loss: 4.2557\n",
      "epoch 0, step 1800/1800: train loss: 0.8402  val loss: 4.2035\n",
      "epoch 0, step 2100/2100: train loss: 0.7183  val loss: 4.1506\n",
      "epoch 0, step 2400/2400: train loss: 0.7372  val loss: 4.1241\n",
      "epoch 0, step 2700/2700: train loss: 0.6976  val loss: 3.9909\n",
      "epoch 0, step 3000/3000: train loss: 0.6467  val loss: 3.9443\n",
      "epoch 0, step 3300/3300: train loss: 0.6356  val loss: 4.0133\n",
      "epoch 0, step 3600/3600: train loss: 0.6404  val loss: 3.9684\n",
      "epoch 0, step 3900/3900: train loss: 0.5941  val loss: 3.8894\n",
      "epoch 0, step 4200/4200: train loss: 0.4952  val loss: 3.8438\n",
      "epoch 0, step 4500/4500: train loss: 0.5147  val loss: 3.7010\n",
      "epoch 0, step 4800/4800: train loss: 0.5179  val loss: 3.6863\n",
      "5000 5000\n",
      "\n",
      "\n",
      "\n",
      "  train  10000 \n",
      "=====================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006999015808105469,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd877bdfdc9485c83d51692e078b61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060007572174072266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 611951,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ad839095f244768d88082d540c208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/611951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 300/300: train loss: 3.3386  val loss: 4.1310\n",
      "epoch 0, step 600/600: train loss: 1.8216  val loss: 4.1946\n",
      "epoch 0, step 900/900: train loss: 1.3261  val loss: 4.2949\n",
      "epoch 0, step 1200/1200: train loss: 1.1007  val loss: 4.2955\n",
      "epoch 0, step 1500/1500: train loss: 0.8992  val loss: 4.1552\n",
      "epoch 0, step 1800/1800: train loss: 0.8030  val loss: 3.9634\n",
      "epoch 0, step 2100/2100: train loss: 0.7577  val loss: 3.9841\n",
      "epoch 0, step 2400/2400: train loss: 0.7195  val loss: 3.8902\n",
      "epoch 0, step 2700/2700: train loss: 0.6773  val loss: 3.7926\n",
      "epoch 0, step 3000/3000: train loss: 0.6472  val loss: 3.6773\n",
      "epoch 0, step 3300/3300: train loss: 0.6116  val loss: 3.6449\n",
      "epoch 0, step 3600/3600: train loss: 0.5773  val loss: 3.6178\n",
      "epoch 0, step 3900/3900: train loss: 0.5002  val loss: 3.5502\n",
      "epoch 0, step 4200/4200: train loss: 0.5624  val loss: 3.6562\n",
      "epoch 0, step 4500/4500: train loss: 0.5160  val loss: 3.5402\n",
      "epoch 0, step 4800/4800: train loss: 0.5009  val loss: 3.3636\n",
      "epoch 0, step 5100/5100: train loss: 0.5149  val loss: 3.3097\n",
      "epoch 0, step 5400/5400: train loss: 0.5022  val loss: 3.1707\n",
      "epoch 0, step 5700/5700: train loss: 0.4839  val loss: 3.1313\n",
      "epoch 0, step 6000/6000: train loss: 0.4727  val loss: 3.1646\n",
      "epoch 0, step 6300/6300: train loss: 0.4659  val loss: 3.1978\n",
      "epoch 0, step 6600/6600: train loss: 0.4302  val loss: 3.2565\n",
      "epoch 0, step 6900/6900: train loss: 0.4461  val loss: 3.1840\n",
      "epoch 0, step 7200/7200: train loss: 0.4160  val loss: 3.1134\n",
      "epoch 0, step 7500/7500: train loss: 0.4194  val loss: 3.1489\n",
      "epoch 0, step 7800/7800: train loss: 0.3867  val loss: 3.1649\n",
      "epoch 0, step 8100/8100: train loss: 0.4234  val loss: 2.9625\n",
      "epoch 0, step 8400/8400: train loss: 0.4001  val loss: 2.9339\n",
      "epoch 0, step 8700/8700: train loss: 0.4299  val loss: 2.9520\n",
      "epoch 0, step 9000/9000: train loss: 0.4186  val loss: 3.0610\n",
      "epoch 0, step 9300/9300: train loss: 0.3732  val loss: 3.0002\n",
      "epoch 0, step 9600/9600: train loss: 0.3722  val loss: 3.0122\n",
      "epoch 0, step 9900/9900: train loss: 0.3747  val loss: 3.0554\n",
      "10000 10000\n",
      "CPU times: total: 22h 57min 22s\n",
      "Wall time: 22h 54min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for steps in [1000, 5000, 10000]:\n",
    "    for dname, d in datasets.items():\n",
    "        print(f'\\n\\n\\n  {dname}  {steps} \\n=====================\\n\\n')\n",
    "        model = train_model(\n",
    "            d['form'].tolist(), \n",
    "            d['data'].tolist(), \n",
    "            model_name=model_name, \n",
    "            batch_size=2, \n",
    "            max_epochs=1000, \n",
    "            max_steps=steps\n",
    "        )\n",
    "        model.save_pretrained(f't5_small_{dname}_{steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTu3zq0XJ5VN"
   },
   "source": [
    "## RuPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:29:29.975610Z",
     "start_time": "2023-05-30T07:29:29.854064Z"
    },
    "id": "jL8RrMWaJ5VN"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17gKK0PELJf1"
   },
   "source": [
    "Используем уже предобученную модель _rugpt3large_based_on_gpt2_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:29:54.451996Z",
     "start_time": "2023-05-30T07:29:33.696603Z"
    },
    "id": "EpPQrylWJ5VN",
    "outputId": "456a400a-497e-42cc-a806-63b81dfcb1f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "backbone_id = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(backbone_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone_id, pad_token=\"<pad>\", eos_token=\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:29:59.060840Z",
     "start_time": "2023-05-30T07:29:58.237145Z"
    },
    "id": "cjUCKK0LJ5VN"
   },
   "outputs": [],
   "source": [
    "from ruprompts import PromptFormat\n",
    "\n",
    "prompt_format = PromptFormat(\"<P*100>{cluster}<P*20>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkgmQlMEJ5VN"
   },
   "source": [
    "Определяем параметризацию обучаемых вложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:02.549617Z",
     "start_time": "2023-05-30T07:30:02.520579Z"
    },
    "id": "_u9HwGuHJ5VN"
   },
   "outputs": [],
   "source": [
    "from ruprompts import TensorPromptProvider\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "prompt_provider = TensorPromptProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QQu0JmJJ5VO"
   },
   "source": [
    "Собираем формат prompt и провайдер prompt в объект prompt и применяем его к модели и токенизатору, т.е. добавляем в токенизатор специальные токены и модифицируем слой входных вложений модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:06.132035Z",
     "start_time": "2023-05-30T07:30:05.978041Z"
    },
    "id": "Nlx3JhcnJ5VO"
   },
   "outputs": [],
   "source": [
    "from ruprompts import Prompt\n",
    "\n",
    "prompt = Prompt(prompt_format, prompt_provider)\n",
    "prompt.patch(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3taWUOxCJ5VO"
   },
   "source": [
    "Начинаем предпроцессинг данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:11.021054Z",
     "start_time": "2023-05-30T07:30:09.577445Z"
    },
    "colab": {
     "referenced_widgets": [
      "36e211c8357042ac8233292e0d7efe6a",
      "35e8079f53e042129823e60e2e5e1613",
      "",
      "1bc815ee7fa64982aa07515fdf5ff3f8"
     ]
    },
    "id": "zj1tZjwAJ5VO",
    "outputId": "b9924565-1ab1-4680-ac14-374265c591ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/user/.cache/huggingface/datasets/csv/default-3cc147501614d040/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00400233268737793,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b63f1cdd1948ae8e958d722e2a8974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('csv', data_files={'train': 'data/clustered_data/taiga_train_data_dbscan__clustered.csv', 'val': 'data/clustered_data/taiga_dev_data_dbscan__clustered.csv'})\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:14.920764Z",
     "start_time": "2023-05-30T07:30:14.423917Z"
    },
    "colab": {
     "referenced_widgets": [
      "0e34ef384bf94903bb087f4a05e50f19"
     ]
    },
    "id": "D60mgzfPJ5VP",
    "outputId": "a3a98db0-500b-4b5a-8aea-4c8ee603277e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-3cc147501614d040\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-4a57d5e770409869.arrow\n",
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-3cc147501614d040\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-4d9cc30a8b522374.arrow\n"
     ]
    }
   ],
   "source": [
    "from ruprompts import Text2TextPreprocessor\n",
    "\n",
    "preprocessor = Text2TextPreprocessor(\n",
    "    prompt_format=prompt_format,\n",
    "    tokenizer=tokenizer,\n",
    "    target_field=\"data\",\n",
    "    max_tokens=1792,\n",
    "    truncation_field=\"cluster\",\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(preprocessor)\n",
    "valid_dataset = val_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJjvvJewLi51"
   },
   "source": [
    "Определяем параметры обучения, выбираем размер батча, количество степов, функции потерь и т.д.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:18.517245Z",
     "start_time": "2023-05-30T07:30:18.488236Z"
    },
    "id": "OBeHv_ehJ5VP"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./try10_05_taiga_dbscan\",\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_steps=1000, #100\n",
    "    save_steps=1000, #100\n",
    "    logging_steps=1000, #100\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=0.1,\n",
    "    max_steps=10000, #100000\n",
    "    report_to=\"tensorboard\",\n",
    "    # report_to=[\"tensorboard\", \"wandb\"],  # uncomment to log to WandB\n",
    "    logging_dir=\"logs\",\n",
    "    seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVtAuPaOJ5VP"
   },
   "source": [
    "Выбираем параметры оптимизации модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:30:22.119592Z",
     "start_time": "2023-05-30T07:30:22.092021Z"
    },
    "id": "-osTo4rMJ5VQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(prompt_provider.parameters(), lr=training_args.learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=training_args.max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02oyEdcrJ5VQ"
   },
   "source": [
    "Начинаем обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T10:35:12.299799Z",
     "start_time": "2023-05-30T07:30:25.641392Z"
    },
    "id": "-iJePP7AJ5VQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 3:04:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.325700</td>\n",
       "      <td>1.202783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.170800</td>\n",
       "      <td>1.076645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.912793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.812440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.825900</td>\n",
       "      <td>0.773553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.790100</td>\n",
       "      <td>0.753797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.718018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.716100</td>\n",
       "      <td>0.697416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>0.694091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>0.684634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.9825136474609375, metrics={'train_runtime': 11085.1302, 'train_samples_per_second': 4.511, 'train_steps_per_second': 0.902, 'total_flos': 3.04247738045952e+16, 'train_loss': 0.9825136474609375, 'epoch': 0.67})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from ruprompts.callbacks import (\n",
    "    FreezeTransformerUnfreezePrompt,\n",
    "    ReduceCheckpoint,\n",
    "    SavePretrainedPrompt,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=preprocessor.collate_fn(),\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[FreezeTransformerUnfreezePrompt(), ReduceCheckpoint(), SavePretrainedPrompt(prompt)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
